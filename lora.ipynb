{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implement LoRA layer"
      ],
      "metadata": {
        "id": "3NWUoDAJw0Cs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super(LoRALayer, self).__init__()\n",
        "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
        "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5)) # follow: https://github.com/microsoft/LoRA/blob/main/loralib/layers.py#L124\n",
        "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.alpha * (x @ self.A @ self.B)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LinearWithLoRA(torch.nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super(LinearWithLoRA, self).__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x) + self.lora(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to replace `Linear` layers with `LinearWithLoRA` layers"
      ],
      "metadata": {
        "id": "gTN1SmlLxCxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def replace_linear_with_lora(model, rank, alpha):\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            # Check if the module is an instance of torch.nn.Linear\n",
        "            # Replace the Linear layer with a LinearWithLoRA layer\n",
        "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
        "        else:\n",
        "            # If the module is not a Linear layer, recursively apply the function to the child modules\n",
        "            replace_linear_with_lora(module, rank, alpha)"
      ],
      "metadata": {
        "id": "98wSOk7PxCBn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply LoRA Layers"
      ],
      "metadata": {
        "id": "2ddttw6Cxkep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel"
      ],
      "metadata": {
        "id": "db-nLg5dxjCP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModel.from_pretrained('microsoft/deberta-v3-large')"
      ],
      "metadata": {
        "id": "3v2o6GuhyIF9",
        "outputId": "c6e053c3-d2e6-40a0-943a-d6d4df018e45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate total number of trainable parameters of the original model"
      ],
      "metadata": {
        "id": "gQYPhgpeymiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params_no_lora = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params_no_lora"
      ],
      "metadata": {
        "id": "nq1Y1an4yX49",
        "outputId": "ad601b9b-9de7-439d-9717-56884a98a6e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "434012160"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freeze All Layers Before Applying LoRA Layers"
      ],
      "metadata": {
        "id": "iNUD__nSytgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "wrInm1yUyO1v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_trainable_params"
      ],
      "metadata": {
        "id": "rNa1ewemy-7_",
        "outputId": "961e4f5d-d971-4602-9416-5a479e450e32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replace Linear Layers"
      ],
      "metadata": {
        "id": "ArlRXjSqza2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replace_linear_with_lora(model=model, rank=8, alpha=16)"
      ],
      "metadata": {
        "id": "vDKWrW40zZBV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params_with_lora = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params_with_lora"
      ],
      "metadata": {
        "id": "uE3utjFJzr5A",
        "outputId": "dbf66fdc-cc49-450c-c536-3ba5139b1666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3538944"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "((total_params_no_lora - total_params_with_lora) / total_params_no_lora) * 100"
      ],
      "metadata": {
        "id": "MlPR7LMxztIJ",
        "outputId": "45520e8a-fbf4-4621-ca7f-067a22c8c679",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.1845979614949"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, applying LoRA reduces the number of trainable parameters by approximately 99.18%.\n",
        "\n",
        "By applying LoRA, we are able to train the model with only about 1% of the original number of parameters. This massive reduction in the number of trainable parameters (approximately 99.18%) means that we can achieve efficient fine-tuning with significantly fewer resources, making the process much more efficient in terms of computation and memory usage."
      ],
      "metadata": {
        "id": "L9ZiLfG_0sQa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u9TOXqkNzwFy"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}